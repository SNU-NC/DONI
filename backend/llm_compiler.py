import os
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatClovaX
from langchain import hub
from typing_extensions import TypedDict
from langgraph.graph import END, StateGraph, START
from typing import Annotated, List, Any, Dict, Optional, AsyncGenerator
from uuid import UUID
from langchain_core.messages import AIMessage
from langchain_core.messages import HumanMessage
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# ë„êµ¬ ì„í¬íŠ¸ ë° ì´ˆê¸°í™” ì½”ë“œ
from tools.webSearch.webSearch_tool import WebSearchTools
from tools.retrieve.analystReport.report_RAG_Tool import ReportRAGTool
from tools.retrieve.financialReport.report_statement_tool import CombinedFinancialReportSearchTool
from tools.math.math_tools import get_math_tool
from tools.financialTerm.fin_knowledge_tools import get_fin_tool
from tools.marketData.market_agent import MarketDataTool
from tools.analyze.stockprice.SameSectorCompareTool import SameSectorAnalyzerTool
from tools.analyze.stockprice.StockAnalyzerTool import StockAnalyzerTool
from tools.analyze.stockprice.CombinedAnalysisTool import CombinedAnalysisTool
from langgraph.graph.message import add_messages

from plan.planner_KB import Planner
from plan.join import create_joiner
from plan.reference import TaskResult, add_task_results

class StreamingEventHandler(BaseCallbackHandler):
    def __init__(self):
        self.events = []
        self.current_step = None
        logger.info("ğŸ”„ StreamingEventHandler ì´ˆê¸°í™”ë¨")
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        logger.debug(f"ğŸš€ LLM ì‹œì‘ - í”„ë¡¬í”„íŠ¸: {prompts[:200]}...")  # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
        step_info = {
            "type": "llm_start",
            "content": "LLMì´ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...",
            "show_in_chat": True
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info("ğŸ’­ LLM ì‹œì‘ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        if hasattr(response, 'generations') and response.generations:
            content = response.generations[0][0].text
        else:
            content = str(response)
        
        logger.debug(f"âœ… LLM ì™„ë£Œ - ì‘ë‹µ: {content[:200]}...")  # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
        
        step_info = {
            "type": "llm_end",
            "content": content,
            "show_in_chat": False
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info("ğŸ“ LLM ì¢…ë£Œ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        tool_name = serialized.get("name", "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬")
        logger.debug(f"ğŸ”§ ë„êµ¬ ì‹œì‘ - {tool_name}: {input_str[:200]}...")
        
        step_info = {
            "type": "tool_start",
            "content": f"ğŸ”§ {tool_name} ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...",
            "tool_name": tool_name,
            "show_in_chat": True
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info(f"ğŸ› ï¸ {tool_name} ë„êµ¬ ì‹œì‘ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        summary = output[:200] + "..." if len(output) > 200 else output
        logger.debug(f"ğŸ¯ ë„êµ¬ ì™„ë£Œ - ê²°ê³¼: {summary}")
        
        step_info = {
            "type": "tool_end",
            "content": f"ğŸ¯ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: {summary}",
            "full_output": output,
            "show_in_chat": True
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info("ğŸ ë„êµ¬ ì¢…ë£Œ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        logger.debug(f"â›“ï¸ ì²´ì¸ ì‹œì‘ - ì…ë ¥: {str(inputs)[:200]}...")
        
        step_info = {
            "type": "chain_start",
            "content": "ìƒˆë¡œìš´ ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...",
            "show_in_chat": False
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info("ğŸ”— ì²´ì¸ ì‹œì‘ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        logger.debug(f"ğŸ”— ì²´ì¸ ì™„ë£Œ - ì¶œë ¥: {str(outputs)[:200]}...")
        
        step_info = {
            "type": "chain_end",
            "content": "ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "show_in_chat": False
        }
        self.events.append(step_info)
        self.current_step = step_info
        logger.info("âœ¨ ì²´ì¸ ì¢…ë£Œ ì´ë²¤íŠ¸ ê¸°ë¡ë¨")

    def get_current_step(self) -> Optional[Dict[str, Any]]:
        if self.current_step:
            logger.debug(f"í˜„ì¬ ë‹¨ê³„ ë°˜í™˜: {self.current_step['type']}")
        return self.current_step

def initialize_chain():
    # LLM ì´ˆê¸°í™”
    llm_4o = ChatOpenAI(model="gpt-4-0125-preview", api_key=os.getenv("OPENAI_API_KEY"))
    llm_mini = ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
    clovaX = ChatClovaX(model="HCX-003", clovastudio_api_key=os.getenv("CLOVA_API_KEY"), temperature=0.1)

    # ë„êµ¬ ì´ˆê¸°í™”
    tools = [
        WebSearchTools(llm_mini),
        get_math_tool(llm_4o),
        ReportRAGTool(),
        StockAnalyzerTool(),
        CombinedAnalysisTool(),
        SameSectorAnalyzerTool(),
        CombinedFinancialReportSearchTool(),
        MarketDataTool()
    ]

    # í”„ë¡¬í”„íŠ¸ì™€ í”Œë˜ë„ˆ ì´ˆê¸°í™”
    prompt = hub.pull("snunc/rag-llm-compiler")
    planner = Planner(llm_4o, llm_mini, clovaX, llm_4o, tools, prompt)
    joiner = create_joiner(llm_4o)  

    # ê·¸ë˜í”„ ìƒíƒœ ì •ì˜
    class State(TypedDict):
        messages: Annotated[list, add_messages]
        replan_count: int
        task_results: Annotated[List[TaskResult], add_task_results]
        key_information: List[str]
        report_agent_use: bool
        output_list: List[str]
    # ê·¸ë˜í”„ ìƒì„± ë° ì„¤ì •
    graph_builder = StateGraph(State)
    graph_builder.add_node("plan_and_schedule", planner.create_plan_and_schedule)
    graph_builder.add_node("join", joiner)
    graph_builder.add_edge("plan_and_schedule", "join")

    def should_continue(state):
        messages = state["messages"]
        replan_count = state["replan_count"]

        if state.get("report_agent_use", True):
            print("report agentë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return END

        if isinstance(messages[-1], AIMessage):
            if replan_count >= 2:
                print("replan_count ì´ê±° ë•Œë¬¸ì— ì£½ìŒ   : ", replan_count)
            return END
        return "plan_and_schedule"

    graph_builder.add_conditional_edges("join", should_continue)
    graph_builder.add_edge(START, "plan_and_schedule")

    return graph_builder.compile() 


class LLMCompiler:
    def __init__(self):
        logger.info("ğŸ”¨ LLMCompiler ì´ˆê¸°í™” ì¤‘...")
        self.chain = initialize_chain()
        logger.info("âœ… LLMCompiler ì´ˆê¸°í™” ì™„ë£Œ")

    async def astream(self, query: str) -> AsyncGenerator[Dict[str, Any], None]:
        logger.info(f"ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì¿¼ë¦¬: {query}")
        event_handler = StreamingEventHandler()
        
        state = {
            "messages": [HumanMessage(content=query)],
            "key_information": [],
        }
        logger.debug(f"ì´ˆê¸° ìƒíƒœ ì„¤ì •: {state}")
        
        try:
            async for chunk in self.chain.astream(
                state,
                config={"callbacks": [event_handler]}
            ):
                logger.debug(f"ì²­í¬ ìˆ˜ì‹ : {str(chunk)[:200]}...")
                
                current_step = event_handler.get_current_step()
                if current_step and current_step.get("show_in_chat"):
                    logger.info(f"ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì „ì†¡: {current_step['type']}")
                    yield {
                        "type": "step",
                        "content": current_step["content"],
                        "step_type": current_step["type"]
                    }
                
                if isinstance(chunk, dict) and "messages" in chunk:
                    final_message = chunk["messages"][-1]
                    if isinstance(final_message, AIMessage):
                        logger.info("ìµœì¢… ì‘ë‹µ ì „ì†¡")
                        yield {
                            "type": "final",
                            "content": final_message.content,
                            "events": event_handler.events
                        }
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    def run(self, query: str) -> dict:
        logger.info(f"ğŸ“ ì‹¤í–‰ ì‹œì‘ - ì¿¼ë¦¬: {query}")
        try:
            event_handler = StreamingEventHandler()
            
            state = {
                "messages": [HumanMessage(content=query)],
                "key_information": [],
            }
            logger.debug(f"ì´ˆê¸° ìƒíƒœ ì„¤ì •: {state}")
            
            result = self.chain.invoke(state, config={"callbacks": [event_handler]})
            logger.debug(f"ì²´ì¸ ì‹¤í–‰ ê²°ê³¼: {str(result)[:200]}...")
            
            final_message = result["messages"][-1]
            
            if isinstance(final_message, AIMessage):
                docs = []
                if "key_information" in result:
                    for info in result["key_information"]:
                        doc = {
                            "tool": info.get("tool", ""),
                            "referenced_content": info.get("referenced_content", ""),
                            "filename": info.get("filename"),
                            "page_number": info.get("page_number"),
                            "link": info.get("link"),
                            "title": info.get("title"),
                            "broker": info.get("broker"),
                            "target_price": info.get("target_price"),
                            "investment_opinion": info.get("investment_opinion"),
                            "analysis_result": info.get("analysis_result", ""),
                            "content": info.get("content", "")
                        }
                        filtered_doc = {k: v for k, v in doc.items() if v not in [None, ""]}
                        if filtered_doc:
                            docs.append(filtered_doc)
                
                logger.info("âœ… ì‹¤í–‰ ì™„ë£Œ - ì„±ê³µ")
                return {
                    "answer": final_message.content,
                    "docs": docs if docs else [],
                    "events": event_handler.events
                }
            else:
                logger.error("âŒ ì‹¤í–‰ ì‹¤íŒ¨ - ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹")
                return {
                    "status": "error",
                    "message": "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    "events": event_handler.events
                }
                    
        except Exception as e:
            logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "status": "error",
                "message": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "events": event_handler.events if 'event_handler' in locals() else []
            } 